import nltk
from nltk import word_tokenize

#s = raw_input("Enter some text: ")
#print "You typed", len(word_tokenize(s)), "words."

"""
Doesn't work due to encoding issues.
raw = open('document.txt').read()
type(raw)
tokens = word_tokenize(raw)
type(tokens)
words = [w.lower() for w in tokens]
type(words)
vocab = sorted(set(words))
type(vocab)
"""
